{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42004ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding用AIモデルの読み込みを開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kotas\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの読み込み完了。\n",
      "\n",
      "論文のEmbedding生成を開始します...\n",
      "02_内山田湖太_卒論_最終版.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "EI_16_田中慶人_卒論v4.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "kaku.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "kazuki.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "mae.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "naka.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "oisi - コピー - コピー.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "oisi - コピー.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "oisi.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "soma - コピー.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "soma.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "自分の卒論.docxを処理中...\n",
      "  -> Embedding生成完了。\n",
      "\n",
      "全ての論文のEmbedding生成が完了し、'papers.json'に保存しました。\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-docx python-dotenv\n",
    "!pip install -U sentence-transformers\n",
    "\n",
    "import os\n",
    "import json\n",
    "import docx\n",
    "import time\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# --- 準備 ---\n",
    "load_dotenv()\n",
    "try:\n",
    "    # APIキーを設定\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"APIキーが.envファイルに設定されていません。\")\n",
    "    genai.configure(api_key=api_key)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# --- 設定値 ---\n",
    "MODEL_FOR_EMBEDDING = 'text-embedding-004'\n",
    "CHUNK_SIZE = 2500 # バイト数上限エラーを避けるため、安全な値に設定\n",
    "\n",
    "# --- 関数定義 ---\n",
    "\n",
    "def get_text_from_docx(filepath):\n",
    "    \"\"\"docxファイルからテキストを抽出する\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(filepath)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"  -> エラー: {filepath} の読み込みに失敗 - {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_key_sections(text):\n",
    "    \"\"\"論文テキストから要旨、はじめに、おわりの部分を抽出する\"\"\"\n",
    "    abstract_kw = r\"要旨|要約|Abstract\"\n",
    "    intro_kw = r\"はじめに|序論|緒言|Introduction\"\n",
    "    conclusion_kw = r\"おわりに|結論|結言|Conclusion\"\n",
    "    \n",
    "    sections = {\"abstract\": \"\", \"intro\": \"\", \"conclusion\": \"\"}\n",
    "    reading_section = None\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        line_stripped = line.strip()\n",
    "        # 正規表現のマッチングでセクションの開始を判定\n",
    "        if re.match(f\"^({abstract_kw})$\", line_stripped, re.IGNORECASE): reading_section = \"abstract\"\n",
    "        elif re.match(f\"^({intro_kw})$\", line_stripped, re.IGNORECASE): reading_section = \"intro\"\n",
    "        elif re.match(f\"^({conclusion_kw})$\", line_stripped, re.IGNORECASE): reading_section = \"conclusion\"\n",
    "        \n",
    "        if reading_section:\n",
    "            sections[reading_section] += line + \"\\n\"\n",
    "            \n",
    "    return (sections[\"abstract\"] + sections[\"intro\"] + sections[\"conclusion\"]).strip()\n",
    "\n",
    "def create_embedding_with_fallback(text, title):\n",
    "    \"\"\"全文 → 主要セクション → チャンク化、という優先順位でEmbeddingを試行する。\"\"\"\n",
    "    \n",
    "    # 1. 全文で試行\n",
    "    try:\n",
    "        print(\"  -> 全文でのEmbedding生成を試行...\")\n",
    "        response = genai.embed_content(model=MODEL_FOR_EMBEDDING, content=text, task_type=\"RETRIEVAL_DOCUMENT\", title=title)\n",
    "        print(\"  -> 全文での生成に成功。\")\n",
    "        return response['embedding']\n",
    "    except exceptions.InvalidArgument as e:\n",
    "        if \"payload size exceeds the limit\" not in str(e):\n",
    "            print(f\"  -> 予期せぬエラー（サイズ上限以外）: {e}\")\n",
    "            return None\n",
    "\n",
    "        # 2. サイズエラーなら、主要セクション抽出を試行\n",
    "        print(\"  -> 全文が長すぎるため、主要セクションの抽出を試行...\")\n",
    "        key_text = extract_key_sections(text)\n",
    "        \n",
    "        content_to_embed = None\n",
    "        source_description = \"\"\n",
    "\n",
    "        if key_text:\n",
    "            print(\"  -> 主要セクションを抽出。これでEmbeddingを生成します。\")\n",
    "            content_to_embed = key_text\n",
    "            source_description = \"主要セクション\"\n",
    "        else:\n",
    "            print(\"  -> 主要セクションが見つからず。最終手段として「最初・真ん中・最後」を抽出します。\")\n",
    "            # 全文をチャンク化の対象とする\n",
    "            content_to_embed = text\n",
    "            source_description = \"全文のチャンク\"\n",
    "\n",
    "        try:\n",
    "            # 3. それでもテキストが長すぎる場合はチャンク化する\n",
    "            if len(content_to_embed) > 15000: # 念のためここでも長さをチェック\n",
    "                 print(f\"  -> 「{source_description}」も長すぎるため、最終手段でチャンク化します。\")\n",
    "                 start_text = content_to_embed[:CHUNK_SIZE]\n",
    "                 mid_point = len(content_to_embed) // 2\n",
    "                 middle_text = content_to_embed[mid_point - (CHUNK_SIZE // 2) : mid_point + (CHUNK_SIZE // 2)]\n",
    "                 end_text = content_to_embed[-CHUNK_SIZE:]\n",
    "                 chunked_text = f\"【論文の冒頭】\\n{start_text}\\n\\n【論文の中間部分】\\n{middle_text}\\n\\n【論文の末尾】\\n{end_text}\"\n",
    "                 content_to_embed = chunked_text\n",
    "                 source_description = \"チャンク版\"\n",
    "\n",
    "            response = genai.embed_content(model=MODEL_FOR_EMBEDDING, content=content_to_embed, task_type=\"RETRIEVAL_DOCUMENT\", title=title)\n",
    "            print(f\"  -> 「{source_description}」での生成に成功。\")\n",
    "            return response['embedding']\n",
    "        except Exception as e_retry:\n",
    "            print(f\"  -> フォールバック処理中にエラーが発生: {e_retry}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  -> 予期せぬエラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 実行部分 ---\n",
    "papers_dir = './papers'\n",
    "all_papers_data = []\n",
    "\n",
    "if not os.path.isdir(papers_dir):\n",
    "    print(f\"エラー: '{papers_dir}' ディレクトリが見つかりません。\")\n",
    "else:\n",
    "    print(\"論文の分析とEmbedding生成を開始します...\")\n",
    "    docx_files = [f for f in os.listdir(papers_dir) if f.endswith('.docx')]\n",
    "    for docx_file in docx_files:\n",
    "        filepath = os.path.join(papers_dir, docx_file)\n",
    "        print(f\"{docx_file}を処理中...\")\n",
    "        text = get_text_from_docx(filepath)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        embedding = create_embedding_with_fallback(text, docx_file)\n",
    "        \n",
    "        if embedding:\n",
    "            all_papers_data.append({\n",
    "                \"filename\": docx_file,\n",
    "                \"content\": text[:500] + \"...\",\n",
    "                \"embedding\": embedding\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  -> {docx_file} のEmbedding生成に最終的に失敗しました。スキップします。\")\n",
    "        \n",
    "        time.sleep(2) \n",
    "\n",
    "# --- 最終結果をJSONファイルに書き出す ---\n",
    "with open('papers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_papers_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n全ての論文のEmbedding生成が完了し、'papers.json'に保存しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
